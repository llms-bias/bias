<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>No LLM is Free From Bias: A Comprehensive Study</title>
    
    <!-- Tailwind CSS for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <style>
        /* Use the Inter font family */
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Add a subtle background pattern */
        body {
            background-color: #f8fafc; /* slate-50 */
        }
        .section-title {
            @apply text-3xl font-bold text-gray-800 mb-8 text-center;
        }
        .card {
            @apply bg-white rounded-xl shadow-md overflow-hidden p-8 border border-gray-200;
        }
    </style>
</head>
<body class="text-gray-700">

    <!-- Header & Navigation -->
    <header class="bg-white/80 backdrop-blur-lg sticky top-0 z-50 border-b border-gray-200">
        <div class="container mx-auto px-6 py-4 flex justify-between items-center">
            <h1 class="text-xl font-bold text-gray-800">Bias in LLMs</h1>
            <nav class="hidden md:flex space-x-8">
                <a href="#abstract" class="text-gray-600 hover:text-blue-600 transition-colors">Abstract</a>
                <a href="#methodology" class="text-gray-600 hover:text-blue-600 transition-colors">Methodology</a>
                <a href="#results" class="text-gray-600 hover:text-blue-600 transition-colors">Results</a>
                <a href="#citation" class="text-gray-600 hover:text-blue-600 transition-colors">Citation</a>
                <a href="https://arxiv.org/pdf/2503.11985" target="_blank" class="font-semibold text-blue-600 hover:text-blue-800 transition-colors">ðŸ“„ Read Paper</a>
                <a href="https://github.com/Pruthwik/bias_eval" target="_blank" class="font-semibold text-blue-600 hover:text-blue-800 transition-colors">ðŸ’» View Code</a>
            </nav>
            <button id="mobile-menu-button" class="md:hidden p-2 rounded-md text-gray-600 hover:bg-gray-100 focus:outline-none">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </div>
        <!-- Mobile Menu -->
        <div id="mobile-menu" class="hidden md:hidden px-6 pb-4">
            <a href="#abstract" class="block py-2 text-gray-600 hover:text-blue-600">Abstract</a>
            <a href="#methodology" class="block py-2 text-gray-600 hover:text-blue-600">Methodology</a>
            <a href="#results" class="block py-2 text-gray-600 hover:text-blue-600">Results</a>
            <a href="#citation" class="block py-2 text-gray-600 hover:text-blue-600">Citation</a>
            <a href="https://arxiv.org/pdf/2503.11985" target="_blank" class="block py-2 font-semibold text-blue-600 hover:text-blue-800">ðŸ“„ Read Paper</a>
            <a href="https://github.com/Pruthwik/bias_eval" target="_blank" class="block py-2 font-semibold text-blue-600 hover:text-blue-800">ðŸ’» View Code</a>
        </div>
    </header>

    <!-- Main Content -->
    <main class="container mx-auto px-6 py-12">

        <!-- Title and Authors -->
        <section class="text-center mb-16">
            <h2 class="text-4xl md:text-5xl font-extrabold text-gray-900 leading-tight mb-4">
                No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models
            </h2>
            <p class="text-lg text-gray-500 mb-8">Published in the ECML-PKDD BIAS 2025 Workshop</p>
            <div class="flex justify-center flex-wrap gap-x-6 gap-y-2 text-lg mb-4">
                <span class="font-medium text-gray-800">Charaka Vinayak Kumar<sup>2</sup></span>
                <span class="font-medium text-gray-800">Ashok Urlana<sup>1,2</sup></span>
                <span class="font-medium text-gray-800">Gopichand Kanumolu<sup>2</sup></span>
                <span class="font-medium text-gray-800">Bala Mallikarjunarao Garlapati<sup>2</sup></span>
                <span class="font-medium text-gray-800">Pruthwik Mishra<sup>3</sup></span>
            </div>
            <div class="flex justify-center flex-wrap gap-x-6 gap-y-1 text-md text-gray-600">
                <span><sup>1</sup>IIIT Hyderabad</span>
                <span><sup>2</sup>TCS Research, Hyderabad, India</span>
                <span><sup>3</sup>SVNIT Surat, India</span>
            </div>
        </section>

        <!-- Abstract Section -->
        <section id="abstract" class="mb-20 card">
             <div class="max-w-4xl mx-auto">
                <h3 class="section-title">Abstract</h3>
                <p class="text-lg leading-relaxed text-gray-700">
                    Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often reflect different forms of bias present in the training data. In the light of this perceived limitation, we provide a unified evaluation of benchmarks using a set of representative LLMs that cover different forms of biases starting from physical characteristics to socio-economic categories. Moreover, we propose five prompting approaches to carry out the bias detection task across different aspects of bias. Further, we formulate three research questions to gain valuable insight in detecting biases in LLMs using different approaches and evaluation metrics across benchmarks. The results indicate that each of the selected LLMs suffer from one or the other form of bias with the LLaMA3.1-8B model being the least biased. Finally, we conclude the paper with the identification of key challenges and possible future directions.
                </p>
                <div class="mt-6 bg-yellow-50 border border-yellow-200 text-yellow-800 text-sm rounded-lg p-4">
                    <strong>Warning:</strong> Some examples in this paper may be offensive or upsetting.
                </div>
            </div>
        </section>

        <!-- Methodology Section -->
        <section id="methodology" class="mb-20">
            <h3 class="section-title">Methodology & Findings</h3>
            <div class="max-w-6xl mx-auto">
                <div class="card mb-12">
                     <h4 class="text-2xl font-bold text-gray-800 mb-6 text-center">Prompting-based Methods for Bias Assessment</h4>
                     <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-8">
                        <div class="bg-gray-50 p-6 rounded-lg border">
                            <h5 class="font-bold text-lg mb-2">1. Mask Prediction (with choice)</h5>
                            <p class="text-sm">Models choose from provided options to fill a `[MASK]` token. Used to evaluate gender, race, religion, profession, etc., with LMS, SS, and ICAT metrics.</p>
                        </div>
                        <div class="bg-gray-50 p-6 rounded-lg border">
                            <h5 class="font-bold text-lg mb-2">2. Mask Prediction (no choice)</h5>
                            <p class="text-sm">Models must generate a word to fill a `[MASK]` token without any options, ideal for evaluating gender bias in co-reference resolution tasks.</p>
                        </div>
                        <div class="bg-gray-50 p-6 rounded-lg border">
                            <h5 class="font-bold text-lg mb-2">3. Question-Answering</h5>
                            <p class="text-sm">Models answer a question based on a context from a given set of choices, revealing biases in how they associate traits with different groups.</p>
                        </div>
                        <div class="bg-gray-50 p-6 rounded-lg border">
                            <h5 class="font-bold text-lg mb-2">4. Association-based</h5>
                            <p class="text-sm">Models are prompted to associate toxic content with demographic groups (gender, race, etc.), quantifying perceived stereotypes.</p>
                        </div>
                        <div class="bg-gray-50 p-6 rounded-lg border">
                            <h5 class="font-bold text-lg mb-2">5. Scoring-based</h5>
                            <p class="text-sm">Models score the intensity of an emotion in a sentence, revealing if emotional perception differs across gender and race.</p>
                        </div>
                     </div>
                </div>
                 <div class="card">
                    <h4 class="text-2xl font-bold text-gray-800 mb-6 text-center">Key Findings on Bias Aspects</h4>
                    <div class="grid md:grid-cols-3 gap-8">
                        <div>
                            <h5 class="font-semibold text-xl mb-2 text-blue-700">Gender Bias</h5>
                            <p>Classical stereotypes persist. Biases are stronger when context is insufficient, involving professions, or negative-toned questions. Models tend to associate toxic content and certain emotional gradients more strongly with males.</p>
                        </div>
                        <div>
                            <h5 class="font-semibold text-xl mb-2 text-blue-700">Religion Bias</h5>
                            <p>LLMs often associate Christian, Sikh, and Buddhist religions with positive-toned questions, while Orthodox and Atheist beliefs are linked to negative tones. Christianity, Islam, and Hinduism are most frequently associated with toxic content.</p>
                        </div>
                        <div>
                            <h5 class="font-semibold text-xl mb-2 text-blue-700">Race Bias</h5>
                            <p>Negative questions are more often associated with Blacks, Native Americans, Asians, and Hispanics, while positive questions are linked to Whites. Whites and Asians are the top two races associated with toxic content.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Key Visual and Contributions -->
        <section id="results" class="mb-20">
            <div class="grid md:grid-cols-2 gap-12 items-center">
                <div class="card">
                    <h4 class="text-2xl font-bold text-gray-800 mb-4">Bias Ranking of LLMs</h4>
                    <!-- TODO: Replace this placeholder with your actual Figure 1. -->
                    <!-- You can name your image 'figure1.png' and place it in the same folder as this index.html file -->
                    <img src="figure1.png" 
                         alt="Chart showing normalized bias rank scores for different LLMs across various bias categories" 
                         class="rounded-lg shadow-md border border-gray-200 w-full"
                         onerror="this.onerror=null;this.src='https://placehold.co/600x400/e2e8f0/475569?text=Image+Not+Found';">
                    <p class="text-sm text-gray-500 mt-2 text-center">
                        Fig 1. The positioning of various LLMs based on their biases. A lower normalized bias rank score is better.
                    </p>
                </div>
                <div class="card">
                    <h4 class="text-2xl font-bold text-gray-800 mb-4">Key Contributions</h4>
                    <ul class="space-y-4 text-lg">
                        <li class="flex items-start">
                            <span class="text-blue-500 font-bold mr-3">1.</span>
                            <span>We provide a <strong>systematic study</strong> to quantify the bias in several representative LLMs across various bias aspects.</span>
                        </li>
                        <li class="flex items-start">
                            <span class="text-blue-500 font-bold mr-3">2.</span>
                            <span>We propose <strong>five different prompting-based approaches</strong> to quantify the bias in LLMs.</span>
                        </li>
                        <li class="flex items-start">
                             <span class="text-blue-500 font-bold mr-3">3.</span>
                            <span>We discuss various <strong>challenges and future directions</strong> to foster further research to design robust bias detection techniques in LLMs.</span>
                        </li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Citation Section -->
        <section id="citation" class="mb-20">
            <div class="max-w-4xl mx-auto card">
                <h3 class="section-title">Citation</h3>
                <p class="text-center text-lg mb-6">If you find our work useful, please consider citing:</p>
                <div class="bg-gray-100 rounded-lg p-6 font-mono text-sm text-gray-800 relative">
                    <button onclick="copyCitation()" class="absolute top-2 right-2 bg-gray-200 hover:bg-gray-300 text-gray-700 font-sans text-xs font-semibold py-1 px-2 rounded-md transition-colors">Copy</button>
                    <pre id="citation-text"><code>@inproceedings{kumar2025nollm,
    title     = {No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models},
    author    = {Kumar, Charaka Vinayak and Urlana, Ashok and Kanumolu, Gopichand and Garlapati, Bala Mallikarjunarao and Mishra, Pruthwik},
    booktitle = {Proceedings of the ECML-PKDD Workshop on Bias and Fairness in AI (BIAS)},
    year      = {2025},
    url       = {https://llms-bias.github.io/bias/}
}</code></pre>
                </div>
                <div id="copy-feedback" class="text-center mt-4 text-green-600 font-semibold opacity-0 transition-opacity">
                    Copied to clipboard!
                </div>
            </div>
        </section>

    </main>

    <!-- Footer -->
    <footer class="bg-gray-800 text-white">
        <div class="container mx-auto px-6 py-8 text-center">
            <p>&copy; 2025 The llms-bias Authors. All Rights Reserved.</p>
            <p class="text-sm text-gray-400 mt-2">This website is hosted on <a href="https://pages.github.com/" target="_blank" class="underline hover:text-gray-200">GitHub Pages</a>.</p>
        </div>
    </footer>

    <script>
        // JavaScript for mobile menu toggle
        const mobileMenuButton = document.getElementById('mobile-menu-button');
        const mobileMenu = document.getElementById('mobile-menu');
        mobileMenuButton.addEventListener('click', () => {
            mobileMenu.classList.toggle('hidden');
        });

        // JavaScript for copying citation
        function copyCitation() {
            const citationText = document.getElementById('citation-text').innerText;
            const feedback = document.getElementById('copy-feedback');
            
            // Create a temporary textarea to hold the text and copy it
            const textArea = document.createElement('textarea');
            textArea.value = citationText;
            document.body.appendChild(textArea);
            textArea.select();
            try {
                document.execCommand('copy');
                feedback.style.opacity = '1';
                setTimeout(() => {
                    feedback.style.opacity = '0';
                }, 2000);
            } catch (err) {
                console.error('Failed to copy text: ', err);
                alert('Failed to copy citation.');
            }
            document.body.removeChild(textArea);
        }
    </script>

</body>
</html>
